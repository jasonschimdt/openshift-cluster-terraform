# OpenShift Inventory Template.
# Note that when the infrastructure is generated by Terraform, this file is
# expanded into './inventory.cfg', based on the rules in:
#
#   ./modules/openshift/08-inventory.tf

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children]
masters
etcd
nodes

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
# SSH user, this user should allow ssh based auth without requiring a password
ansible_ssh_user=ec2-user

# If ansible_ssh_user is not root, ansible_become must be set to true
ansible_become=true

# Deploy OKD 3.11.
openshift_deployment_type=origin
openshift_release=v3.11

openshift_install_examples=true
openshift_metrics_install_metrics=true
openshift_console_install=true
openshift_logging_install_logging=false

openshift_check_min_host_memory_gb=8
openshift_disable_check=docker_image_availability,docker_storage,disk_availability,memory_availability

# We need a wildcard DNS setup for our public access to services
openshift_master_default_subdomain=${default_subdomain}
openshift_public_hostname=${public_hostname}

# Use an htpasswd file as the indentity provider.
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# Set the cluster_id.
openshift_clusterid=${cluster_id}

#is this needed???
#openshift_hosted_registry_selector='node-role.kubernetes.io/infra=true'
#openshift_logging_es_ops_nodeselector={"node-role.kubernetes.io/infra":"true"}
#openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra":"true"}

# Define the standard set of node groups, as per:
#   https://github.com/openshift/openshift-ansible#node-group-definition-and-mapping
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true']}, {'name': 'node-config-master-infra', 'labels': ['node-role.kubernetes.io/infra=true,node-role.kubernetes.io/master=true']}, {'name': 'node-config-all-in-one', 'labels': ['node-role.kubernetes.io/infra=true,node-role.kubernetes.io/master=true,node-role.kubernetes.io/compute=true']}]

# Create the masters host group. Note that due do:
#   https://github.com/dwmkerr/terraform-aws-openshift/issues/40
# We cannot use the internal DNS names (such as master.openshift.local) as there
# is a bug with the installer when using the AWS cloud provider.
# Note that we use the master node as an infra node as well, which is not recommended for production use.
[masters]
${master_hostname}

# host group for etcd
[etcd]
${master_hostname}

# all nodes - along with their openshift_node_groups.
[nodes]
${master_hostname} openshift_node_group_name='node-config-master-infra' openshift_schedulable=true
${node1_hostname} openshift_node_group_name='node-config-compute'
${node2_hostname} openshift_node_group_name='node-config-compute'
